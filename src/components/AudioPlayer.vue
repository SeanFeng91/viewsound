<template>
  <div class="bg-white/5 backdrop-blur-sm rounded-lg border border-white/10 p-4">
    <h3 class="text-lg font-semibold text-white mb-4">éŸ³é¢‘æ–‡ä»¶å¤„ç†</h3>
    
    <!-- éŸ³é¢‘æ–‡ä»¶é€‰æ‹© -->
    <div class="mb-4">
      <label class="block text-sm font-medium text-white mb-1">é€‰æ‹©éŸ³é¢‘æ–‡ä»¶</label>
      <input 
        ref="audioFileInput"
        type="file" 
        accept="audio/*" 
        @change="handleFileChange"
        class="w-full px-3 py-2 bg-white/10 border border-white/20 rounded-lg text-white file:mr-4 file:py-1 file:px-3 file:border-0 file:text-sm file:font-medium file:bg-blue-500 file:text-white file:rounded file:cursor-pointer hover:file:bg-blue-600"
      />
    </div>
    
    <!-- å¤„ç†çŠ¶æ€æ˜¾ç¤º -->
    <div v-if="isProcessing" class="mb-4 p-3 bg-blue-500/10 border border-blue-500/20 rounded-lg">
      <div class="flex items-center text-blue-400">
        <div class="animate-spin rounded-full h-4 w-4 border-b-2 border-blue-400 mr-2"></div>
        æ­£åœ¨åˆ†æéŸ³é¢‘æ–‡ä»¶...
      </div>
    </div>
    
    <!-- éŸ³é¢‘ä¿¡æ¯æ˜¾ç¤º -->
    <div v-if="audioInfo && audioBuffer" class="mb-4 p-3 bg-green-500/10 border border-green-500/20 rounded-lg">
      <div class="flex items-center text-green-400 mb-2">
        <span class="mr-2">âœ“</span>
        éŸ³é¢‘æ–‡ä»¶å°±ç»ª
      </div>
      <div class="text-sm text-gray-300 space-y-1">
        <div>æ–‡ä»¶: {{ audioInfo.filename }}</div>
        <div>æ—¶é•¿: {{ audioInfo.duration?.toFixed(2) }}ç§’</div>
        <div>é‡‡æ ·ç‡: {{ audioInfo.sampleRate }}Hz</div>
        <div>å£°é“æ•°: {{ audioInfo.channels }}</div>
      </div>
    </div>
    
    <!-- æ³¢å½¢æ˜¾ç¤º -->
    <div v-if="audioBuffer" class="mb-4">
      <div class="block text-sm font-medium text-white mb-2">æ³¢å½¢æ˜¾ç¤º</div>
      <div 
        ref="waveformContainer"
        class="h-40 relative overflow-hidden bg-white/5 rounded-lg border border-white/10"
      >
        <canvas 
          ref="waveformCanvas"
          class="absolute inset-0 w-full h-full"
          @mousemove="onWaveformHover"
        ></canvas>
        <!-- æ’­æ”¾ä½ç½®æŒ‡ç¤ºå™¨ -->
        <div 
          v-if="isPlaying && duration > 0"
          class="absolute top-0 bottom-0 w-0.5 bg-red-500 pointer-events-none transition-all duration-100"
          :style="{ left: progressPercentage + '%' }"
        ></div>
      </div>
    </div>
    
    <!-- é¢‘è°±æ˜¾ç¤º -->
    <div v-if="audioBuffer" class="mb-4">
      <div class="block text-sm font-medium text-white mb-2">é¢‘è°±åˆ†æ</div>
      <div 
        ref="spectrumContainer"
        class="h-32 bg-white/5 rounded-lg border border-white/10"
      >
        <canvas 
          ref="spectrumCanvas"
          class="w-full h-full"
        ></canvas>
      </div>
    </div>
    
    <!-- é¢‘ç‡åˆ†æä¿¡æ¯ -->
    <div v-if="isPlaying && isExcitationMode" class="mb-4 p-3 bg-white/5 rounded-lg">
      <div class="flex justify-between items-center mb-2">
        <span class="text-sm font-medium text-white">å®æ—¶é¢‘ç‡åˆ†æ</span>
        <button 
          @click="resetFrequencyAnalysis"
          class="px-2 py-1 text-xs bg-gray-600 hover:bg-gray-700 text-white rounded transition-colors"
        >
          é‡ç½®åˆ†æ
        </button>
      </div>
      <div class="text-xs text-gray-300 space-y-1">
        <div v-if="currentAnalysis">
          <span class="text-blue-400">ä¸»å¯¼é¢‘ç‡:</span> 
          {{ currentAnalysis.dominantFrequency.toFixed(1) }}Hz
          <span class="ml-2 text-yellow-400">ç½®ä¿¡åº¦:</span> 
          {{ (currentAnalysis.confidence * 100).toFixed(0) }}%
        </div>
        <div v-if="currentAnalysis && currentAnalysis.peaks.length > 1">
          <span class="text-green-400">ä¸»è¦å³°å€¼:</span>
          <span v-for="(peak, index) in currentAnalysis.peaks.slice(0, 3)" :key="index" class="ml-1">
            {{ peak.frequency.toFixed(0) }}Hz
          </span>
        </div>
      </div>
    </div>
    
    <!-- æç¤ºä¿¡æ¯ -->
    <div v-if="!audioBuffer && !isProcessing" class="p-3 bg-gray-500/10 border border-gray-500/20 rounded-lg">
      <p class="text-sm text-gray-400">
        ğŸ’¡ ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶åï¼Œå°†è‡ªåŠ¨è¿›è¡Œåˆ†æå¤„ç†ã€‚éŸ³é¢‘æ’­æ”¾æ§åˆ¶é€šè¿‡ä¸»æ§åˆ¶é¢æ¿çš„"å¼€å§‹/æš‚åœ"æŒ‰é’®è¿›è¡Œã€‚
      </p>
    </div>
  </div>
</template>

<script setup>
import { ref, computed, onMounted, onUnmounted, watch } from 'vue'

// å“åº”å¼æ•°æ®
const audioFileInput = ref(null)
const waveformContainer = ref(null)
const waveformCanvas = ref(null)
const spectrumContainer = ref(null)
const spectrumCanvas = ref(null)

const selectedFile = ref(null)
const audioBuffer = ref(null)
const audioInfo = ref(null)
const isProcessing = ref(false)
const isPlaying = ref(false)
const currentTime = ref(0)
const duration = ref(0)
const isAudioEnabled = ref(true)
const isExcitationMode = ref(false)
const currentAnalysis = ref(null) // å½“å‰çš„é¢‘ç‡åˆ†æç»“æœ

// éŸ³é¢‘ç›¸å…³
let audioContext = null
let audioSource = null
let analyser = null
let gainNode = null
let startTime = 0
let pauseTime = 0
let onFrequencyChange = null // é¢‘ç‡å˜åŒ–å›è°ƒå‡½æ•°
let frequencyHistory = [] // é¢‘ç‡å†å²è®°å½•ï¼Œç”¨äºå¹³æ»‘
let lastDominantFreq = 0 // ä¸Šä¸€æ¬¡çš„ä¸»å¯¼é¢‘ç‡

// åŠ¨ç”»å¸§
let animationFrame = null
let waveformCtx = null
let spectrumCtx = null

// è®¡ç®—å±æ€§
const progressPercentage = computed(() => {
  if (duration.value === 0) return 0
  return (currentTime.value / duration.value) * 100
})

// ç”Ÿå‘½å‘¨æœŸ
onMounted(async () => {
  await initAudioContext()
  initCanvas()
})

onUnmounted(() => {
  if (animationFrame) {
    cancelAnimationFrame(animationFrame)
  }
  if (audioSource) {
    audioSource.disconnect()
  }
})

// æ–¹æ³•
async function initAudioContext() {
  try {
    audioContext = new (window.AudioContext || window.webkitAudioContext)()
    
    // åˆ›å»ºåˆ†æå™¨èŠ‚ç‚¹
    analyser = audioContext.createAnalyser()
    analyser.fftSize = 2048
    analyser.smoothingTimeConstant = 0.8
    
    // åˆ›å»ºå¢ç›ŠèŠ‚ç‚¹
    gainNode = audioContext.createGain()
    gainNode.connect(audioContext.destination)
    
    console.log('AudioContext initialized successfully')
  } catch (error) {
    console.error('Failed to initialize AudioContext:', error)
  }
}

function initCanvas() {
  if (waveformCanvas.value) {
    waveformCtx = waveformCanvas.value.getContext('2d')
    waveformCanvas.value.width = waveformContainer.value.clientWidth
    waveformCanvas.value.height = waveformContainer.value.clientHeight
  }
  
  if (spectrumCanvas.value) {
    spectrumCtx = spectrumCanvas.value.getContext('2d')
    spectrumCanvas.value.width = spectrumContainer.value.clientWidth
    spectrumCanvas.value.height = spectrumContainer.value.clientHeight
  }
}

// å¤„ç†æ–‡ä»¶é€‰æ‹©
async function handleFileChange(event) {
  const file = event.target.files[0]
  if (!file) return
  
  selectedFile.value = file
  audioInfo.value = {
    filename: file.name,
    size: file.size,
    type: file.type
  }
  
  console.log('ğŸ“ é€‰æ‹©éŸ³é¢‘æ–‡ä»¶:', file.name)
  
  // è‡ªåŠ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶
  await processAudio()
}

async function processAudio() {
  if (!selectedFile.value) return
  
  isProcessing.value = true
  
  try {
    const arrayBuffer = await selectedFile.value.arrayBuffer()
    audioBuffer.value = await audioContext.decodeAudioData(arrayBuffer)
    
    // æ›´æ–°éŸ³é¢‘ä¿¡æ¯
    audioInfo.value = {
      ...audioInfo.value,
      duration: audioBuffer.value.duration,
      sampleRate: audioBuffer.value.sampleRate,
      channels: audioBuffer.value.numberOfChannels
    }
    
    duration.value = audioBuffer.value.duration
    
    // ç»˜åˆ¶å®Œæ•´æ³¢å½¢
    drawWaveform()
    
    console.log('Audio processed successfully')
  } catch (error) {
    console.error('Error processing audio:', error)
    alert('éŸ³é¢‘å¤„ç†å¤±è´¥: ' + error.message)
  } finally {
    isProcessing.value = false
  }
}

async function togglePlayback() {
  if (!audioBuffer.value) return
  
  if (audioContext.state === 'suspended') {
    await audioContext.resume()
  }
  
  if (isPlaying.value) {
    pausePlayback()
  } else {
    startPlayback()
  }
}

function startPlayback() {
  if (!audioBuffer.value) return
  
  // åœæ­¢å½“å‰æ’­æ”¾
  if (audioSource) {
    audioSource.disconnect()
  }
  
  // é‡ç½®é¢‘ç‡åˆ†æçŠ¶æ€
  frequencyHistory = []
  lastDominantFreq = 0
  
  // åˆ›å»ºæ–°çš„éŸ³é¢‘æº
  audioSource = audioContext.createBufferSource()
  audioSource.buffer = audioBuffer.value
  
  // è¿æ¥éŸ³é¢‘å›¾
  audioSource.connect(analyser)
  analyser.connect(gainNode)
  
  // è®¾ç½®ç»“æŸå›è°ƒ
  audioSource.onended = () => {
    if (currentTime.value >= duration.value) {
      stopPlayback()
    }
  }
  
  // å¼€å§‹æ’­æ”¾
  const offset = pauseTime > 0 ? pauseTime : 0
  audioSource.start(0, offset)
  startTime = audioContext.currentTime - offset
  isPlaying.value = true
  
  // å¼€å§‹åŠ¨ç”»å¾ªç¯
  updateProgress()
  
  console.log('Playback started')
}

function pausePlayback() {
  if (audioSource) {
    audioSource.stop()
    audioSource.disconnect()
    audioSource = null
  }
  
  pauseTime = currentTime.value
  isPlaying.value = false
  
  if (animationFrame) {
    cancelAnimationFrame(animationFrame)
    animationFrame = null
  }
  
  console.log('Playback paused')
}

function stopPlayback() {
  if (audioSource) {
    audioSource.stop()
    audioSource.disconnect()
    audioSource = null
  }
  
  isPlaying.value = false
  currentTime.value = 0
  pauseTime = 0
  startTime = 0
  
  if (animationFrame) {
    cancelAnimationFrame(animationFrame)
    animationFrame = null
  }
  
  // é‡æ–°ç»˜åˆ¶å®Œæ•´æ³¢å½¢
  drawWaveform()
  
  console.log('Playback stopped')
}

function updateProgress() {
  if (!isPlaying.value) return
  
  currentTime.value = audioContext.currentTime - startTime
  
  if (currentTime.value >= duration.value) {
    stopPlayback()
    return
  }
  
  // ç»˜åˆ¶æ»šåŠ¨æ³¢å½¢
  drawScrollingWaveform()
  
  // ç»˜åˆ¶å®æ—¶é¢‘è°±
  drawSpectrum()
  
  // å¦‚æœåœ¨æ¿€åŠ±æ¨¡å¼ï¼Œåˆ†æå¹¶ä¼ é€’ä¸»å¯¼é¢‘ç‡
  if (isExcitationMode.value && onFrequencyChange) {
    const analysis = getMusicalFrequencyAnalysis()
    if (analysis && analysis.confidence > 0.4) { // åªä¼ é€’ç½®ä¿¡åº¦è¾ƒé«˜çš„é¢‘ç‡
      onFrequencyChange(analysis.frequency)
    }
    
    // æ›´æ–°åˆ†æç»“æœæ˜¾ç¤º
    currentAnalysis.value = analysis
  }
  
  animationFrame = requestAnimationFrame(updateProgress)
}

function drawWaveform() {
  if (!waveformCtx || !audioBuffer.value) return
  
  const canvas = waveformCanvas.value
  const ctx = waveformCtx
  const width = canvas.width
  const height = canvas.height
  
  ctx.clearRect(0, 0, width, height)
  
  // è·å–éŸ³é¢‘æ•°æ®
  const channelData = audioBuffer.value.getChannelData(0)
  const samples = channelData.length
  const step = Math.ceil(samples / width)
  
  ctx.strokeStyle = '#3b82f6'
  ctx.lineWidth = 1
  ctx.beginPath()
  
  for (let i = 0; i < width; i++) {
    const sampleIndex = i * step
    const sample = channelData[sampleIndex] || 0
    const y = (sample + 1) * height / 2
    
    if (i === 0) {
      ctx.moveTo(i, y)
    } else {
      ctx.lineTo(i, y)
    }
  }
  
  ctx.stroke()
}

function drawScrollingWaveform() {
  if (!waveformCtx || !audioBuffer.value) return
  
  const canvas = waveformCanvas.value
  const ctx = waveformCtx
  const width = canvas.width
  const height = canvas.height
  
  ctx.clearRect(0, 0, width, height)
  
  // è®¡ç®—å½“å‰æ˜¾ç¤ºçš„æ—¶é—´çª—å£ï¼ˆæ˜¾ç¤ºå‰åå„2ç§’ï¼‰
  const windowSize = 4 // ç§’
  const startTime = Math.max(0, currentTime.value - windowSize / 2)
  const endTime = Math.min(duration.value, currentTime.value + windowSize / 2)
  
  const sampleRate = audioBuffer.value.sampleRate
  const channelData = audioBuffer.value.getChannelData(0)
  const startSample = Math.floor(startTime * sampleRate)
  const endSample = Math.floor(endTime * sampleRate)
  const totalSamples = endSample - startSample
  
  if (totalSamples <= 0) return
  
  const step = Math.ceil(totalSamples / width)
  
  // ç»˜åˆ¶æ³¢å½¢
  ctx.strokeStyle = '#3b82f6'
  ctx.lineWidth = 1
  ctx.beginPath()
  
  for (let i = 0; i < width; i++) {
    const sampleIndex = startSample + i * step
    if (sampleIndex >= channelData.length) break
    
    const sample = channelData[sampleIndex] || 0
    const y = (sample + 1) * height / 2
    
    if (i === 0) {
      ctx.moveTo(i, y)
    } else {
      ctx.lineTo(i, y)
    }
  }
  
  ctx.stroke()
  
  // ç»˜åˆ¶å½“å‰æ’­æ”¾ä½ç½®çº¿
  const currentPosition = ((currentTime.value - startTime) / (endTime - startTime)) * width
  ctx.strokeStyle = '#ef4444'
  ctx.lineWidth = 2
  ctx.beginPath()
  ctx.moveTo(currentPosition, 0)
  ctx.lineTo(currentPosition, height)
  ctx.stroke()
}

function drawSpectrum() {
  if (!spectrumCtx || !analyser) return
  
  const canvas = spectrumCanvas.value
  const ctx = spectrumCtx
  const width = canvas.width
  const height = canvas.height
  
  const bufferLength = analyser.frequencyBinCount
  const dataArray = new Uint8Array(bufferLength)
  analyser.getByteFrequencyData(dataArray)
  
  ctx.clearRect(0, 0, width, height)
  
  const barWidth = width / bufferLength
  let x = 0
  
  // ç»˜åˆ¶é¢‘è°±æ¡
  for (let i = 0; i < bufferLength; i++) {
    const barHeight = (dataArray[i] / 255) * height
    
    // é¢œè‰²æ¸å˜
    const hue = (i / bufferLength) * 300
    ctx.fillStyle = `hsl(${hue}, 100%, 50%)`
    
    ctx.fillRect(x, height - barHeight, barWidth, barHeight)
    x += barWidth
  }
}

function seekTo(event) {
  if (!audioBuffer.value) return
  
  const rect = event.target.getBoundingClientRect()
  const clickX = event.clientX - rect.left
  const percentage = clickX / rect.width
  const newTime = percentage * duration.value
  
  currentTime.value = newTime
  pauseTime = newTime
  
  if (isPlaying.value) {
    // é‡æ–°å¼€å§‹æ’­æ”¾
    pausePlayback()
    startPlayback()
  }
}

function formatTime(seconds) {
  const mins = Math.floor(seconds / 60)
  const secs = Math.floor(seconds % 60)
  return `${mins}:${secs.toString().padStart(2, '0')}`
}

function onWaveformHover(event) {
  // å¯ä»¥æ·»åŠ é¼ æ ‡æ‚¬åœæ˜¾ç¤ºæ—¶é—´ä¿¡æ¯çš„åŠŸèƒ½
}

// éŸ³é¢‘æ¿€åŠ±ç›¸å…³æ–¹æ³•
function startAudioExcitation() {
  if (!audioBuffer.value || !isAudioEnabled.value) {
    console.warn('æ— éŸ³é¢‘ç¼“å†²åŒºæˆ–éŸ³é¢‘å·²ç¦ç”¨ï¼Œæ— æ³•å¼€å§‹æ¿€åŠ±')
    return false
  }
  
  isExcitationMode.value = true
  
  // å¦‚æœä¸åœ¨æ’­æ”¾çŠ¶æ€ï¼Œå¼€å§‹æ’­æ”¾
  if (!isPlaying.value) {
    startPlayback()
  }
  
  console.log('ğŸµ å¼€å§‹éŸ³é¢‘æ¿€åŠ±æ¨¡å¼')
  return true
}

function stopAudioExcitation() {
  isExcitationMode.value = false
  
  // åœæ­¢æ’­æ”¾
  if (isPlaying.value) {
    stopPlayback()
  }
  
  console.log('â¹ï¸ åœæ­¢éŸ³é¢‘æ¿€åŠ±æ¨¡å¼')
}

function setAudioEnabled(enabled) {
  isAudioEnabled.value = enabled
  
  // å¦‚æœç¦ç”¨éŸ³é¢‘ä¸”æ­£åœ¨æ¿€åŠ±æ¨¡å¼ï¼Œåœæ­¢æ¿€åŠ±
  if (!enabled && isExcitationMode.value) {
    stopAudioExcitation()
  }
  
  console.log('ğŸ”Š éŸ³é¢‘æ¿€åŠ±', enabled ? 'å¯ç”¨' : 'ç¦ç”¨')
}

// è·å–å½“å‰éŸ³é¢‘çš„ä¸»è¦é¢‘ç‡æˆåˆ†ï¼ˆç”¨äºæŒ¯åŠ¨åˆ†æï¼‰
function getAudioFrequencyData() {
  if (!analyser || !isPlaying.value) return null
  
  const bufferLength = analyser.frequencyBinCount
  const dataArray = new Uint8Array(bufferLength)
  analyser.getByteFrequencyData(dataArray)
  
  // æ‰¾åˆ°ä¸»è¦é¢‘ç‡æˆåˆ†
  const sampleRate = audioContext.sampleRate
  const frequencies = []
  const threshold = 50 // é¢‘ç‡å¼ºåº¦é˜ˆå€¼
  
  for (let i = 0; i < bufferLength; i++) {
    if (dataArray[i] > threshold) {
      const frequency = (i * sampleRate) / (2 * bufferLength)
      frequencies.push({
        frequency: frequency,
        amplitude: dataArray[i] / 255.0
      })
    }
  }
  
  // æŒ‰å¹…åº¦æ’åºï¼Œè¿”å›å‰5ä¸ªä¸»è¦é¢‘ç‡
  return frequencies
    .sort((a, b) => b.amplitude - a.amplitude)
    .slice(0, 5)
}

// è®¾ç½®é¢‘ç‡å˜åŒ–å›è°ƒå‡½æ•°
function setFrequencyChangeCallback(callback) {
  onFrequencyChange = callback
}

// æ”¹è¿›çš„é¢‘ç‡åˆ†æç®—æ³•
function getMusicalFrequencyAnalysis() {
  if (!analyser || !isPlaying.value) return null
  
  const bufferLength = analyser.frequencyBinCount
  const dataArray = new Uint8Array(bufferLength)
  analyser.getByteFrequencyData(dataArray)
  
  const sampleRate = audioContext.sampleRate
  const freqResolution = sampleRate / (2 * bufferLength)
  
  // 1. æ‰¾åˆ°æ‰€æœ‰æ˜¾è‘—çš„é¢‘ç‡å³°å€¼
  const peaks = findFrequencyPeaks(dataArray, freqResolution)
  
  // 2. è¿‡æ»¤æ‰ä½é¢‘å™ªå£°å’Œé«˜é¢‘å™ªå£°
  const filteredPeaks = peaks.filter(peak => 
    peak.frequency >= 60 && peak.frequency <= 4000 && peak.amplitude > 0.1
  )
  
  if (filteredPeaks.length === 0) return null
  
  // 3. å°è¯•è¯†åˆ«åŸºé¢‘
  const fundamentalFreq = findFundamentalFrequency(filteredPeaks)
  
  // 4. å¦‚æœæ‰¾ä¸åˆ°åŸºé¢‘ï¼Œä½¿ç”¨èƒ½é‡æœ€å¤§çš„é¢‘ç‡
  const dominantFreq = fundamentalFreq || filteredPeaks[0].frequency
  
  // 5. åº”ç”¨æ—¶é—´å¹³æ»‘
  const smoothedFreq = applyFrequencySmoothing(dominantFreq)
  
  return {
    frequency: smoothedFreq,
    confidence: fundamentalFreq ? 0.8 : 0.5,
    peaks: filteredPeaks.slice(0, 3) // è¿”å›å‰3ä¸ªä¸»è¦å³°å€¼
  }
}

// å¯»æ‰¾é¢‘ç‡å³°å€¼
function findFrequencyPeaks(dataArray, freqResolution) {
  const peaks = []
  const threshold = 20 // æœ€å°å¹…åº¦é˜ˆå€¼
  
  for (let i = 2; i < dataArray.length - 2; i++) {
    const amplitude = dataArray[i]
    
    // å±€éƒ¨æœ€å¤§å€¼æ£€æµ‹
    if (amplitude > threshold &&
        amplitude > dataArray[i-1] && 
        amplitude > dataArray[i+1] &&
        amplitude > dataArray[i-2] && 
        amplitude > dataArray[i+2]) {
      
      const frequency = i * freqResolution
      peaks.push({
        frequency: frequency,
        amplitude: amplitude / 255.0,
        bin: i
      })
    }
  }
  
  // æŒ‰å¹…åº¦æ’åº
  return peaks.sort((a, b) => b.amplitude - a.amplitude)
}

// è¯†åˆ«åŸºé¢‘ï¼ˆè€ƒè™‘è°æ³¢å…³ç³»ï¼‰
function findFundamentalFrequency(peaks) {
  if (peaks.length < 2) return null
  
  // å°è¯•æ¯ä¸ªå³°å€¼ä½œä¸ºåŸºé¢‘
  for (let i = 0; i < Math.min(peaks.length, 5); i++) {
    const candidateFreq = peaks[i].frequency
    
    // æ£€æŸ¥æ˜¯å¦æœ‰è°æ³¢æ”¯æŒ
    let harmonicSupport = 0
    let totalHarmonicStrength = 0
    
    for (let harmonic = 2; harmonic <= 6; harmonic++) {
      const harmonicFreq = candidateFreq * harmonic
      const tolerance = candidateFreq * 0.05 // 5% å®¹å·®
      
      // å¯»æ‰¾æ¥è¿‘è°æ³¢é¢‘ç‡çš„å³°å€¼
      const nearbyPeak = peaks.find(peak => 
        Math.abs(peak.frequency - harmonicFreq) < tolerance
      )
      
      if (nearbyPeak) {
        harmonicSupport++
        totalHarmonicStrength += nearbyPeak.amplitude
      }
    }
    
    // å¦‚æœæœ‰è¶³å¤Ÿçš„è°æ³¢æ”¯æŒï¼Œè®¤ä¸ºæ˜¯åŸºé¢‘
    if (harmonicSupport >= 2 && candidateFreq >= 80 && candidateFreq <= 2000) {
      return candidateFreq
    }
  }
  
  // å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„åŸºé¢‘ï¼Œè¿”å›æœ€å¼ºçš„ä½é¢‘æˆåˆ†
  const lowFreqPeak = peaks.find(peak => peak.frequency >= 80 && peak.frequency <= 800)
  return lowFreqPeak ? lowFreqPeak.frequency : null
}

// é¢‘ç‡å¹³æ»‘å¤„ç†
function applyFrequencySmoothing(newFreq) {
  const maxHistoryLength = 5
  
  // æ·»åŠ åˆ°å†å²è®°å½•
  frequencyHistory.push(newFreq)
  if (frequencyHistory.length > maxHistoryLength) {
    frequencyHistory.shift()
  }
  
  // å¦‚æœé¢‘ç‡å˜åŒ–å¤ªå‰§çƒˆï¼Œè¿›è¡Œå¹³æ»‘
  if (lastDominantFreq > 0) {
    const freqChange = Math.abs(newFreq - lastDominantFreq) / lastDominantFreq
    
    if (freqChange > 0.3) { // å¦‚æœå˜åŒ–è¶…è¿‡30%
      // ä½¿ç”¨åŠ æƒå¹³å‡å¹³æ»‘
      const weights = [0.4, 0.3, 0.2, 0.1] // æ–°çš„æƒé‡æ›´å¤§
      let weightedSum = 0
      let totalWeight = 0
      
      for (let i = 0; i < Math.min(frequencyHistory.length, weights.length); i++) {
        const idx = frequencyHistory.length - 1 - i
        weightedSum += frequencyHistory[idx] * weights[i]
        totalWeight += weights[i]
      }
      
      newFreq = weightedSum / totalWeight
    }
  }
  
  lastDominantFreq = newFreq
  return newFreq
}

// è·å–å½“å‰éŸ³é¢‘çš„ä¸»å¯¼é¢‘ç‡ï¼ˆæ”¹è¿›ç‰ˆï¼‰
function getDominantFrequency() {
  const analysis = getMusicalFrequencyAnalysis()
  return analysis ? {
    frequency: analysis.frequency,
    amplitude: analysis.peaks[0]?.amplitude || 0,
    confidence: analysis.confidence
  } : null
}

// ç›‘å¬çª—å£å¤§å°å˜åŒ–
watch([waveformContainer, spectrumContainer], () => {
  initCanvas()
  if (audioBuffer.value) {
    drawWaveform()
  }
})

// è·å–è¯¦ç»†çš„éŸ³é¢‘åˆ†æä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
function getDetailedAudioAnalysis() {
  const analysis = getMusicalFrequencyAnalysis()
  if (!analysis) return null
  
  return {
    dominantFrequency: analysis.frequency,
    confidence: analysis.confidence,
    peaks: analysis.peaks,
    frequencyHistory: [...frequencyHistory],
    analysisTime: new Date().toISOString()
  }
}

// æ‰‹åŠ¨é‡ç½®é¢‘ç‡åˆ†æçŠ¶æ€
function resetFrequencyAnalysis() {
  frequencyHistory = []
  lastDominantFreq = 0
  console.log('ï¿½ï¿½ é¢‘ç‡åˆ†æçŠ¶æ€å·²é‡ç½®')
}

// æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶
function hasAudioFile() {
  return !!(audioBuffer.value && selectedFile.value)
}

// æš´éœ²æ–¹æ³•ä¾›çˆ¶ç»„ä»¶è°ƒç”¨
defineExpose({
  startAudioExcitation,
  stopAudioExcitation,
  setAudioEnabled,
  getAudioFrequencyData,
  setFrequencyChangeCallback,
  getDominantFrequency,
  getDetailedAudioAnalysis,
  resetFrequencyAnalysis,
  hasAudioFile
})
</script> 